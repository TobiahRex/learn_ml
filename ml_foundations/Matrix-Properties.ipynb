{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Properties\n",
    "\n",
    "Matrix properties are important to understand when working with matrices. They can help us understand the behavior of matrices and how they can be manipulated. In this notebook, we will discuss some of the most important properties of matrices. The key use-case for Matrices are to solve systems of linear equations, and to represent linear transformations. More concretely, they're functional purpose is to be like operators on vectors, transforming them in some way. Vectors are typically represented as column matrices, and the transformation is represented as a square matrix. When we multiply a matrix by a vector, we are applying the transformation to the vector. When we multiply a matrix by another matrix, we are applying the transformation represented by the second matrix to the transformation represented by the first matrix, this will result in a new transformation ~ a new operator definition that can be applied to vectors. So you could think of matrices as dynamic operators, that can be combined to create new operators.\n",
    "\n",
    "## Visualizing Matrices\n",
    "\n",
    "We can visualize a matrix by\n",
    "\n",
    "## Matrix as Operators\n",
    "\n",
    "1. **Scaling Vectors**: Diagonal matrices can be used to scale vectors. If we multiply a vector by a diagonal matrix, the vector will be scaled by the diagonal elements of the matrix. The axis of the vector will be scaled by the corresponding diagonal element of the matrix.\n",
    "2. **Rotating Vectors**: Rotation matrices can be used to rotate vectors. If we multiply a vector by a rotation matrix, the vector will be rotated by the angle specified by the rotation matrix.\n",
    "3. **Shearing Vectors**: Shearing matrices can be used to shear vectors. If we multiply a vector by a shearing matrix, the vector will be sheared by the amount specified by the shearing matrix. \"Shearing\" is a transformation that distorts the shape of the vector, by pushing the vector in a particular direction. Visually it looks like the vector is being pushed in a particular direction, and the direction of the push is determined by the shearing matrix.\n",
    "4. **Reflection**: Reflection matrices can be used to reflect vectors (or invert them). If we multiply a vector by a reflection matrix, the vector will be reflected across the axis specified by the reflection matrix.\n",
    "5. **Projection**: Projection matrices can be used to project vectors onto lower-dimensional space. If we multiply a vector by a projection matrix, the vector will be projected onto the axis specified by the projection matrix.\n",
    "6. **Translation**: Translation matrices can be used to translate vectors. If we multiply a vector by a translation matrix, the vector will be translated by the amount specified by the translation matrix. \"Translation\" is a transformation that moves the vector in a particular direction.\n",
    "\n",
    "## Matrix Properties\n",
    "\n",
    "1. The Frobenuis Norm\n",
    "2. Matrix Multiplication\n",
    "3. Identity Matrices\n",
    "4. Scalar Matrix\n",
    "5. Symmetric Matrices\n",
    "6. Matrix Inversion\n",
    "7. Diagonal Matrices\n",
    "8. Orthogonal Matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.The Frobenius Norm\n",
    "\n",
    "The overall size of a matrix\n",
    "\n",
    "### 1.WHAT\n",
    "\n",
    "The Frobenius norm of a matrix is the square root of the sum of the absolute squares of its elements. It is also known as the Euclidean norm. It is defined as:\n",
    "$$ \\|A\\|_F = \\sqrt{\\sum_{i=1}^{m} \\sum*{j=1}^{n} |a*{ij}|^2} $$\n",
    "where $a_{ij}$ are the elements of the matrix $A$.\n",
    "\n",
    "### 1.WHY\n",
    "\n",
    "The Frobenius norm is a measure of the size of a matrix. It is used to measure the error in approximating a matrix by another matrix. It is also used to measure the distance between two matrices. The Frobenius norm is used in many machine learning algorithms, such as principal component analysis (PCA) and singular value decomposition (SVD).\n",
    "\n",
    "### 1.VISUALIZATION\n",
    "\n",
    "We can start building our mental model by imagining a matrix as a grid of numbers. The Frobenius norm is the square root of the sum of the squares of all the numbers in the matrix. It is a measure of the size of the matrix as a quantity of space. It helps us understand how much space the matrix occupies in the vector space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Matrix Multiplication\n",
    "\n",
    "Matrix multiplication is a binary operation that produces a matrix from two matrices. For matrix multiplication, the number of columns in the first matrix must be equal to the number of rows in the second matrix. The resulting matrix, known as the matrix product, has the number of rows of the first and the number of columns of the second matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Scalar Matrix\n",
    "\n",
    "Scales/Stretches a Matrix\n",
    "\n",
    "### 3.1.WHAT\n",
    "\n",
    "Allows us to scale a matrix by a scalar. A scalar matrix is a matrix in which all the diagonal elements are equal to a scalar, and all the off-diagonal elements are equal to zero. A scalar matrix is denoted by $\\lambda I$, where $\\lambda$ is the scalar and $I$ is the identity matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagonal Matrix\n",
    "Diagonal Matrices are a special case of scalar matrices. They are matrices in which all the off-diagonal elements are equal to zero. A diagonal matrix is denoted by $D = diag(d_1, d_2, ..., d_n)$, where $d_1, d_2, ..., d_n$ are the diagonal elements of the matrix. It's purpose is to transform a vector by transforming the vector on each axis by a different amount. This is useful for scaling vectors, reflecting vectors, shrinking, expanding, etc in different directions by different amounts. For example, if we have a vector in 2D space, and we want to scale the x-axis by 2 and the y-axis by 3, we can use a diagonal matrix with the elements [2, 3]. When we multiply the vector by this matrix, the x-axis will be scaled by 2 and the y-axis will be scaled by 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal Matrix\n",
    "\n",
    "### WHAT\n",
    "\n",
    "Orthogonal matrices are square matrices with real entries whose columns and rows are orthogonal unit vectors. More directly, their purpose is to be applied to a vector, and whatever the result, there will be no change in the length of the vector(s), nor a change in the angle between the vectors. This means that the matrix is a rotation, reflection, or a combination of both. So if we want to physically move a vector, we can use an orthogonal matrix to do so, and the vector will not change in length or angle. There's also something called a \"determinant\", which is a measure of how much the matrix scales the space. If the determinant is 1, then the matrix is a proper orthogonal matrix, and if the determinant is -1, then the matrix is an improper orthogonal matrix. When we said earlier that the matrix doesn't change the length of the vector, we were referring to proper orthogonal matrices. Improper orthogonal matrices do change the length of the vector, but they do not change the angle between the vectors. This can be visualized as a reflection, where the vector is flipped on some axis (x, y, z), but the angle between the vectors is preserved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity Matrix\n",
    "\n",
    "Contains 1's on the main diagonal and 0's elsewhere. It is a square matrix, and is denoted by $I_n$ where $n$ is the number of rows and columns in the matrix. The identity matrix is a special case of a diagonal matrix, where all the diagonal elements are 1. The identity matrix when applied as an operator on a vector, simply produce the same vector. This is particularly useful when trying to solve linear equations. The identity matrix is also used to define the inverse of a matrix. The inverse of a matrix is a matrix that when multiplied by the original matrix, results in the identity matrix. The identity matrix is also used to define the determinant of a matrix. The determinant of a matrix is a scalar value that is used to determine the invertibility of a matrix. If the determinant of a matrix is zero, the matrix is not invertible. If the determinant of a matrix is non-zero, the matrix is invertible. Intuitively we can think of this type of matrix as a particularly useful quality of a matrix we're concerned with, and knowing whether a matrix has this quality is important for many operations we might want to perform on it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       " array([[1., 0.],\n",
       "        [0., 2.],\n",
       "        [0., 0.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       " array([[3.],\n",
       "        [4.]], dtype=float32)>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "D1 = tf.constant([[1, 0], [0, 2], [0, 0]], dtype=tf.float32)\n",
    "v1 = tf.constant([[3], [4]], dtype=tf.float32)\n",
    "\n",
    "D1, v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
       "array([[3.],\n",
       "       [8.],\n",
       "       [0.]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1 = tf.matmul(D1, v1)\n",
    "result1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvalues and Eigenvectors\n",
    "An Eigenvector is a vector that does not change its direction when a linear transformation is applied to it. An Eigenvalue is a scalar that represents how the Eigenvector was scaled during the transformation. They are useful for understanding the behavior of a matrix, and for understanding the behavior of a linear transformation. For example the Eigenvector can be thought of as the axis of rotation, and the Eigenvalue can be thought of as the amount of rotation, or the angle of rotation. Regarding Machine Learning and Deep Learning, knowing the Eigenvalue and Eigenvector of a matrix can help us solve problems such as Principal Component Analysis (PCA), Singular Value Decomposition (SVD), and many other problems.\n",
    "\n",
    "## Topics\n",
    "1. Affine Transformations\n",
    "2. Eigenvectors and Eigenvalues\n",
    "3. Matrix Determinants\n",
    "4. Eigendecomposition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udemy-data-science-3HlboHdm-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
